# Redis

## Redis 数据结构

|               |                               |                                                              |
| ------------- | ----------------------------- | ------------------------------------------------------------ |
| Redis数据结构 | 底层实现                      | 应用场景                                                     |
| `String`      | 整数（只有 long 类型） 或 SDS | 二进制安全，缓存静态文件、用作计数器（incr），统计次数（如网站访问）。 |
| `Hash`        | 字典 或 压缩列表              | 存储对象，用户信息、商品信息等。                             |
| `List`        | 双端链表 或 压缩列表 或 快表  | 消息队列、粉丝列表、关注列表等。                             |
| `Set`         | 字典 或 整数集合              | 用于去重相关操作                                             |
| `ZSet`        | 字典 或 压缩列表              | 在`Set`功能的基础之外，构建优先队列。                        |
| HyperLoglog   |                               | 基数统计                                                     |
| BloomFilter   |                               | 大数据判断是否存在、解决缓存穿透、爬虫/邮箱等系统的过滤。    |
| GeoHash       |                               | 地里定位，附近的人、附近的 xxx                               |
| bitMap        |                               |                                                              |
| Pub/Sub       |                               |                                                              |
| Stream        |                               |                                                              |

## Redis数据结构原理

1、在 Redis 中， 一个字符串对象除了可以保存字符串值之外，还可以保存 `long` 类型的值，当字符串对象保存的是字符串时，它包含的才是 sds 值，否则的话，它就是一个 `long` 类型的值

```c
typedef char *sds;
struct sdshdr {
    // buf 已占用长度 
    int len;
    // buf 剩余可用长度 
    int free;
    // 实际保存字符串数据的地方
    char buf[]; 
};
```

2、双端链表作为一种通用的数据结构，在 Redis 内部使用得非常多:它既是 Redis `列表结构`的底层实现之一，还被大量 Redis 模块所使用，用于构建 Redis 的其他功能，双端链表还被很多 Redis 内部模块所应用:

- 事务模块使用双端链表来按顺序保存输入的命令;
- 服务器模块使用双端链表来保存多个客户端;
- 订阅/发送模块使用双端链表来保存订阅模式的多个客户端;
- 事件模块使用双端链表来保存时间事件(time event).



## 跳跃表

### 为什么使用跳跃表

首先，因为 zset 要支持随机的插入和删除，所以它 **不宜使用数组来实现**，关于排序问题，我们也很容易就想到 **红黑树/ 平衡树** 这样的树形结构，为什么 Redis 不使用这样一些结构呢？

1. **性能考虑：** 在高并发的情况下，树形结构需要执行一些类似于 rebalance 这样的可能涉及整棵树的操作，相对来说跳跃表的变化只涉及局部 *(下面详细说)*；
2. **实现考虑：** 在复杂度与红黑树相同的情况下，跳跃表实现起来更简单，看起来也更加直观；

基于以上的一些考虑，Redis 基于 **William Pugh** 的论文做出一些改进后采用了 **跳跃表** 这样的结构。

### 本质是解决查找问题

我们先来看一个普通的链表结构：

![图片](https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H5ZLiaicqeR9mzkQuQLwvtFfQJ2WzK4Dj4ibLKst3qkVLjMQcy5FkqLu7pmHhvoH15H5JUmIsBjicdhvw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们需要这个链表按照 score 值进行排序，这也就意味着，当我们需要添加新的元素时，我们需要定位到插入点，这样才可以继续保证链表是有序的，通常我们会使用 **二分查找法**，但二分查找是有序数组的，链表没办法进行位置定位，我们除了遍历整个找到第一个比给定数据大的节点为止 *（时间复杂度 O(n))* 似乎没有更好的办法。

但假如我们每相邻两个节点之间就增加一个指针，让指针指向下一个节点，如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H5ZLiaicqeR9mzkQuQLwvtFfQZYuqmprbyribHqEdWvMKjdeiaaUR5swWG3iciaJ2ghlwp6ubdfxLahIQVg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这样所有新增的指针连成了一个新的链表，但它包含的数据却只有原来的一半 *（图中的为 3，11）*。

现在假设我们想要查找数据时，可以根据这条新的链表查找，如果碰到比待查找数据大的节点时，再回到原来的链表中进行查找，比如，我们想要查找 7，查找的路径则是沿着下图中标注出的红色指针所指向的方向进行的：

![图片](https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H5ZLiaicqeR9mzkQuQLwvtFfQn3GcsgglqK0DaME5KXiciaQLCkkSVKMia9gmv5icavhQhOwuHRavTLIMdg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这是一个略微极端的例子，但我们仍然可以看到，通过新增加的指针查找，我们不再需要与链表上的每一个节点逐一进行比较，这样改进之后需要比较的节点数大概只有原来的一半。

利用同样的方式，我们可以在新产生的链表上，继续为每两个相邻的节点增加一个指针，从而产生第三层链表：

![图片](https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H5ZLiaicqeR9mzkQuQLwvtFfQZxrtjNvic9S9GPVcQiaWS4dhxvCJPxdHSxSCUdP81SU5o6JjS0E9sy5A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在这个新的三层链表结构中，我们试着 **查找 13**，那么沿着最上层链表首先比较的是 11，发现 11 比 13 小，于是我们就知道只需要到 11 后面继续查找，**从而一下子跳过了 11 前面的所有节点。**

可以想象，当链表足够长，这样的多层链表结构可以帮助我们跳过很多下层节点，从而加快查找的效率。

### 更进一步的跳跃表

**跳跃表 skiplist** 就是受到这种多层链表结构的启发而设计出来的。按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到 *O(logn)*。

但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的 2:1 的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点 *（也包括新插入的节点）* 重新进行调整，这会让时间复杂度重新蜕化成 *O(n)*。删除数据也有同样的问题。

![图片](https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H5ZLiaicqeR9mzkQuQLwvtFfQ5qUqf8c0vC3bfbc710Tz6iadcOlDYb39pApOUP9pCaUDQtuicUn9Jibvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从上面的创建和插入的过程中可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点并不会影响到其他节点的层数，因此，**插入操作只需要修改节点前后的指针，而不需要对多个节点都进行调整**，这就降低了插入操作的复杂度。

**Redis 跳跃表默认允许最大的层数是 ZSKIPLIST_MAXLEVEL = 32**



## Redis 内存回收策略

- `volatile-lru`：从已设置过期时间的数据集（`server.db[i].expires`）中挑选最久未使用的数据淘汰；
- `volatile-ttl`：从已设置过期时间的数据集（`server.db[i].expires`）中挑选将要过期的数据淘汰；
- `volatile-random`：从已设置过期时间的数据集（`server.db[i].expires`）中随机选取数据淘汰；
- `allkeys-lru`：当内存不足以容纳新写入数据时，在键空间中，移除最久未使用的 `key`；
- `allkeys-random`：从数据集（`server.db[i].dict`）中随机选取数据淘汰；
- `no-eviction`：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。

`volatile-xxx` 这三个淘汰策略使用的不是全量数据，有可能无法淘汰出足够的内存空间。在没有过期键或者没有设置超时属性的键的情况下，这三种策略和 `noeviction` 差不多。

4.0 版本后增加以下两种：

- `volatile-lfu`：从已设置过期时间的数据集(`server.db[i].expires`)中挑选最不经常使用的数据淘汰。
- `allkeys-lfu`：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 `key`。

一般的经验规则：

- 使用 `allkeys-lru` 策略：当预期请求符合一个幂次分布(二八法则等)，比如一部分的子集元素比其它元素被访问的更多时，可以选择这个策略。
- 使用 `allkeys-random` 策略：循环连续的访问所有的键时，或者预期请求分布平均（所有元素被访问的概率都差不多）。
- 使用 `volatile-ttl`：要采取这个策略，缓存对象的 `TTL` 值最好有差异。

`volatile-lru` 和 `volatile-random` 策略，当想要使用**单一**的 `Redis` 实例来同时实现**缓存淘汰和持久化**一些经常使用的键集合时很有用。

- 对未设置过期时间的键进行持久化保存，对设置了过期时间的键参与缓存淘汰。
- 不过一般运行两个实例是解决这个问题的更好方法。

为键设置过期时间也是需要消耗内存的，所以使用 `allkeys-lru` 这种策略更加节省空间，因为这种策略下可以不为键设置过期时间。

##  **过期键的删除策略**有两种：

- 惰性删除：每次从键空间获取键时，都检查键是否过期，如果过期的话就删除该键，否则返回该键。
- 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。

## **Redis LRU 配置参数：**

- `maxmemory`：配置 `Redis` 存储数据时指定限制的内存大小，比如 `100m`。当缓存消耗的内存超过这个数值时, 将触发数据淘汰。该数据配置为 00 时，表示缓存的数据量没有限制, 即 LRULRU 功能不生效。6464 位的系统默认值为 00，3232 位的系统默认内存限制为 3GB3GB。
- `maxmemory_policy`：触发数据淘汰后的淘汰策略。
- `maxmemory_samples`：随机采样的精度，也就是随即取出 keykey 的数目。该数值配置越大, 越接近于真实的 LRULRU 算法，但是数值越大，相应消耗也变高，对性能有一定影响，样本值默认为 55 。

## 分布式锁实现与分析

### SETNX实现的分布式锁

```she
# 将key设置值为value，如果key不存在，这种情况下等同SET命令。 当key存在时，什么也不做，SET if Not eXists
SETNX key value
```

#### 加锁步骤

1. `SETNX lock.foo <current Unix time + lock timeout + 1>`

   如果客户端获得锁，`SETNX`返回`1`，加锁成功。

   如果`SETNX`返回`0`，那么该键已经被其他的客户端锁定。

2. 接上一步，`SETNX`返回`0`加锁失败，此时，调用`GET lock.foo`获取时间戳检查该锁是否已经过期：

   - 如果没有过期，则休眠一会重试。

   - 如果已经过期，则可以获取该锁。具体的：调用`GETSET lock.foo <current Unix timestamp + lock timeout + 1>`基于当前时间设置新的过期时间。

     **注意**: 这里设置的时候因为在`SETNX`与`GETSET`之间有个窗口期，在这期间锁可能已被其他客户端抢去，所以这里需要判断`GETSET`的返回值，他的返回值是SET之前旧的时间戳：

     - 若旧的时间戳已过期，则表示加锁成功。
     - 若旧的时间戳还未过期（说明被其他客户端抢去并设置了时间戳），代表加锁失败，需要等待重试。

#### 解锁步骤

解锁相对简单，只需`GET lock.foo`时间戳，判断是否过期，过期就调用删除`DEL lock.foo`



### SET实现的分布式锁

```shell
SET key value [EX seconds|PX milliseconds] [NX|XX]
```

#### 加锁步骤

一条命令即可加锁: `SET resource_name my_random_value NX PX 30000`

这个命令只有当`key` 对应的键不存在resource_name时（NX选项的作用）才生效，同时设置30000毫秒的超时，成功设置其值为my_random_value，这是个在所有redis客户端加锁请求中全局唯一的随机值。

#### 解锁步骤

解锁时需要确保my_random_value和加锁的时候一致。下面的Lua脚本可以完成

```lau
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

这段Lua脚本在执行的时候要把前面的`my_random_value`作为`ARGV[1]`的值传进去，把`resource_name`作为`KEYS[1]`的值传进去。释放锁其实包含三步操作：’GET’、判断和’DEL’，用Lua脚本来实现能保证这三步的原子性。



### Redis集群分布式锁Redlock

前面两种分布式锁的实现都是针对单redis master实例，既不是有互为备份的slave节点也不是多master集群，如果是redis集群，每个redis master节点都是独立存储，这种场景用前面两种加锁策略有锁的安全性问题。

比如下面这种场景：

> 1. 客户端1从Master获取了锁。
> 2. Master宕机了，存储锁的key还没有来得及同步到Slave上。
> 3. Slave升级为Master。
> 4. 客户端2从新的Master获取到了对应同一个资源的锁。
>
> 于是，客户端1和客户端2同时持有了同一个资源的锁。锁的安全性被打破。

针对这种多redis服务实例的场景，redis作者antirez设计了**Redlock**

#### 加锁步骤

集群加锁的总体思想是尝试锁住所有节点，当有一半以上节点被锁住就代表加锁成功。集群部署你的数据可能保存在任何一个redis服务节点上，一旦加锁必须确保集群内任意节点被锁住，否则也就失去了加锁的意义。

1. 获取当前时间（毫秒数）。
2. 按顺序依次向N个Redis节点执行**获取锁**的操作。这个获取操作跟前面基于单Redis节点的**获取锁**的过程相同，包含随机字符串`my_random_value`，也包含过期时间(比如`PX 30000`，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个**获取锁**的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）。
3. 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（>= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。
4. 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。
5. 如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起**释放锁**的操作（即前面介绍的Redis Lua脚本）。

#### 解锁步骤

客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。



## Redis分布式锁的缺陷

**1、 redis服务器时钟漂移问题**

如果redis服务器的机器时钟发生了向前跳跃，就会导致这个key过早超时失效，比如说客户端1拿到锁后，key的过期时间是12:02分，但redis服务器本身的时钟比客户端快了2分钟，导致key在12:00的时候就失效了，这时候，如果客户端1还没有释放锁的话，就可能导致多个客户端同时持有同一把锁的问题。

**2、节点发生崩溃重启的话，还是有可能出现多个客户端同时获取锁的情况**

假设一共有5个Redis节点：A、B、C、D、E，客户端1和2分别加锁

1. 客户端1成功锁住了A，B，C，获取锁成功（但D和E没有锁住）。
2. 节点C的master挂了，然后锁还没同步到slave，slave升级为master后丢失了客户端1加的锁。
3. 客户端2这个时候获取锁，锁住了C，D，E，获取锁成功。

> 总结

鱼和熊掌不可兼得，之所以用Redis作为分布式锁的工具，很大程度上是因为Redis本身效率高且单进程的特点，即使在高并发的情况下也能很好的保证性能，但很多时候，性能和安全不能完全兼顾，如果你一定要保证锁的安全性的话，可以用其他的中间件如db、zookeeper来做控制，这些工具能很好的保证锁的安全，但性能方面只能说是差强人意



## 布隆过滤器

> 背景
>
> 举一个场景吧，比如你 **刷抖音**
>
> 你有 **刷到过重复的推荐内容** 吗？这么多的推荐内容要推荐给这么多的用户，它是怎么保证每个用户在看推荐内容时，保证不会出现之前已经看过的推荐视频呢？也就是说，抖音是如何实现 **推送去重** 的呢？
>
> 你会想到服务器 **记录** 了用户看过的 **所有历史记录**，当推荐系统推荐短视频时会从每个用户的历史记录里进行 **筛选**，过滤掉那些已经存在的记录。问题是当 **用户量很大**，每个用户看过的短视频又很多的情况下，这种方式，推荐系统的去重工作 **在性能上跟的上么**
>
> 实际上，如果历史记录存储在关系数据库里，去重就需要频繁地对数据库进行 `exists` 查询，当系统并发量很高时，数据库是很难抗住压力的。
>
> 你可能又想到了 **缓存**，但是这么多用户这么多的历史记录，如果全部缓存起来，那得需要 **浪费多大的空间**

### 原理

**布隆过滤器(Bloom Filter)** 就是这样一种专门用来解决去重问题的高级数据结构

布隆过滤器是一个 bit 向量或者说 bit 数组，长这样：

![img](https://pic3.zhimg.com/80/v2-530c9d4478398718c15632b9aa025c36_720w.jpg)

如果我们要映射一个值到布隆过滤器中，我们需要使用**多个不同的哈希函数**生成**多个哈希值)，**并对每个生成的哈希值指向的 bit 位置 1，例如针对值 “baidu” 和三个不同的哈希函数分别生成了哈希值 1、4、7，则上图转变为：

![img](https://pic4.zhimg.com/80/v2-a0ee721daf43f29dd42b7d441b79d227_720w.jpg)

Ok，我们现在再存一个值 “tencent”，如果哈希函数返回 3、4、8 的话，图继续变为：

![img](https://pic3.zhimg.com/80/v2-c0c20d8e06308aae1578c16afdea3b6a_720w.jpg)

假设再查下"taobao"，哈希返回的是1、5、8，根据5bit位看没有，那这个关键字肯定没有。

> **结论：当布隆过滤器说某个值存在时，这个值 可能不存在；当它说不存在时，那么 一定不存在**



### 布隆过滤器的基本用法

- `bf.add`：添加元素到布隆过滤器中，类似于集合的`sadd`命令，不过`bf.add`命令只能一次添加一个元素，如果想一次添加多个元素，可以使用`bf.madd`命令
- `bf.exists`：判断某个元素是否在过滤器中，类似于集合的`sismember`命令，不过`bf.exists`命令只能一次查询一个元素，如果想一次查询多个元素，可以使用`bf.mexists`命令

```shell
127.0.0.1:6379> bf.add codehole user1
(integer) 1
127.0.0.1:6379> bf.add codehole user2
(integer) 1
127.0.0.1:6379> bf.add codehole user3
(integer) 1
127.0.0.1:6379> bf.exists codehole user1
(integer) 1
127.0.0.1:6379> bf.exists codehole user2
(integer) 1
127.0.0.1:6379> bf.exists codehole user3
(integer) 1
127.0.0.1:6379> bf.exists codehole user4
(integer) 0
127.0.0.1:6379> bf.madd codehole user4 user5 user6
1) (integer) 1
2) (integer) 1
3) (integer) 1
127.0.0.1:6379> bf.mexists codehole user4 user5 user6 user7
1) (integer) 1
2) (integer) 1
3) (integer) 1
4) (integer) 0
```

Redis 也提供了可以自定义参数的布隆过滤器，只需要在 `add` 之前使用 `bf.reserve` 指令显式创建就好了，`bf.reserve` 有三个参数，分别是 `key`、`error_rate` *(错误率)* 和 `initial_size`，默认的 `error_rate` 是 `0.01`，默认的 `initial_size` 是 `10`

- **`error_rate` 越低，需要的空间越大**，对于不需要过于精确的场合，设置稍大一些也没有关系，比如上面说的推送系统，只会让一小部分的内容被过滤掉，整体的观看体验还是不会受到很大影响的；
- **`initial_size` 表示预计放入的元素数量**，当实际数量超过这个值时，误判率就会提升，所以需要提前设置一个较大的数值避免超出导致误判率升高；



### 常见问题

#### 缓存雪崩问题

![缓存雪崩](https://tva1.sinaimg.cn/large/007S8ZIlly1ghsnwdnqolj30tv0ft0zg.jpg)

另外对于 **"Redis 挂掉了，请求全部走数据库"** 这样的情况，有如下的思路：

- **事发前**：实现 Redis 的高可用(主从架构 + Sentinel 或者 Redis Cluster)，尽量避免 Redis 挂掉这种情况发生。
- **事发中**：万一 Redis 真的挂了，我们可以设置本地缓存(ehcache) + 限流(hystrix)，尽量避免我们的数据库被干掉(起码能保证我们的服务还是能正常工作的)
- **事发后**：Redis 持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据。



#### 缓存穿透问题

![缓存穿透](https://tva1.sinaimg.cn/large/007S8ZIlly1ghsnxrax6cj30tw0ft7aq.jpg)



#### 缓存击穿

- 是什么：在高并发的系统中，大量的请求同时查询一个 `key`(热点 `key`) 时，此时这个 `key` 正好失效了，就会导致大量的请求都打到数据库上面去。
- 为什么：`key` 设置了过期时间，`key` 又为热点`key`
- 怎么解决：在第一个查询数据的请求上使用一个 **互斥锁(mutex)** 来锁住它。其他的线程走到这一步拿不到锁就等着，等第一个线程查询到了数据，然后做缓存。后面的线程进来发现已经有缓存了，就直接走缓存。

 ```java
public String get(key) {
    String value = redis.get(key);
    if (value == null) { // 代表缓存值过期
        // 设置 3min 的超时，防止 del 操作失败的时候，下次缓存过期一直不能load db
        if (redis.setnx(key_mutex, 1, 3 * 60) == 1) {  //代表设置成功
            value = db.get(key);
            redis.set(key, value, expire_secs);
            redis.del(key_mutex);
        } else {  
            // 这个时候代表同时候的其他线程已经load db并回设到缓存了
            // 这时候重试获取缓存值即可
            sleep(50);
            get(key);  //重试
        }
    } else return value;
 }
 ```

> 这种 **互斥锁(mutex) \** 在单机情况下是有效的，但是在分布式情况下就要使用**分布式锁**。
>
> 本地锁，只能锁住当前线程，所以需要分布式锁。
>
> Spring Boot 所有的组件在容器中默认都是单例的，使用 `synchronized (this)` 可以实现加锁。
>
> 在每一个微服务中的`synchronized(this)`加锁的对象只是当前实例，但是并未对其他微服务的实例产生影响，即使每个微服务加锁后只允许一个请求，假如有 8 个微服务，仍然会有 8 个线程存在。



#### 缓存和数据库双写一致性问题

![缓存和数据库双写一致性问题](https://tva1.sinaimg.cn/large/007S8ZIlly1ghsnznkhp6j30tw0fqgt6.jpg)

一般我们都是采取**删除缓存**缓存策略的，原因如下：

1. 高并发环境下，无论是先操作数据库还是后操作数据库而言，如果加上更新缓存，那就**更加容易**导致数据库与缓存数据不一致问题。(删除缓存**直接和简单**很多)
2. 如果每次更新了数据库，都要更新缓存【这里指的是频繁更新的场景，这会耗费一定的性能】，倒不如直接删除掉。等再次读取时，缓存里没有，那我到数据库找，在数据库找到再写到缓存里边(体现**懒加载**)

两种策略各自有优缺点：

- 先删除缓存，再更新数据库

- - 在高并发下表现不如意，在原子性被破坏时表现优异

- 先更新数据库，再删除缓存(`Cache Aside Pattern`设计模式)

- - 在高并发下表现优异，在原子性被破坏时表现不如意

> **总结**
>
> - 如果是**用户维度**数据(订单数据、用户数据)，这种并发几率非常小，不用考虑这个问题，缓存数据加 上过期时间，每隔一段时间触发读的主动更新即可。
> - 如果是菜单，商品介绍等**基础数据**，也可以去使用 `canal` 订阅 `binlog` 的方式。
> - **缓存数据 + 过期时间** 足够解决大部分业务对于缓存的要求。
> - 通过**加锁**保证并发**读写、写写**的时候按顺序排好队，读读无所谓，所以适合使用读写锁。(业务不关心脏数据，允许临时脏数据可忽略。)
> - 能放入缓存的数据本就不应该是实时性、一致性要求超高的。所以缓存数据的时候加上过期时间，保证每天拿到当前最新数据即可。
> - 不应该过度设计，增加系统的复杂性。
> - 遇到实时性、一致性要求高的数据，就应该实时直接查数据库，即使这样速度相对于缓存会比较慢。



阿里的canal

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zdGF0aWMubG92ZWJpbGliaWxpLmNvbS9waWMvY2FuYWxfc3l0LnBuZw?x-oss-process=image/format,png)

刷新缓存过程

1、canal作为从节点读取mysl增量binlog

2、通过后台服务监听，把增量数据写进kafka

3、通过队列方式更新缓存，保证一致性
